{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic Datasets for Testing\n",
    "\n",
    "In order to test the performance of equilibration detection methods, we create sets of synthetic datasets modelled on long ABFE runs for six different systems (see the 30 ns non-adaptive runs [here](https://chemrxiv.org/engage/chemrxiv/article-details/6670b524c9c6a5c07aafa972) for details).\n",
    "\n",
    "This notebook is organised as follows:\n",
    "\n",
    "- [Load the data](#load)\n",
    "- [Fit the data](#fit)\n",
    "    - [Model trends with exponentials](#exp)\n",
    "    - [Model stationary distributions with uncorrelated Gaussian noise](#noise)\n",
    "    - [Reintroduce desired correlation structure with Cholesky decomposition](#cholesky)\n",
    "- [Create synthetic datasets](#synth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import red\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from pymbar import timeseries\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from scipy.linalg import toeplitz\n",
    "import seaborn as sns\n",
    "\n",
    "# Set ggplot style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def get_subplots(systems: list[str]) -> tuple[plt.Figure, list[plt.Axes]]:\n",
    "    # Plot two columns side-by-side\n",
    "    n_cols = min(2, len(systems))\n",
    "    n_rows = int(np.ceil(len(systems) / n_cols))\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3*n_rows))\n",
    "    if len(systems) == 1:\n",
    "        axs = [axs]\n",
    "    else:\n",
    "        axs = axs.flatten()\n",
    "\n",
    "    # Set titles\n",
    "    for i, system in enumerate(systems):\n",
    "        axs[i].set_title(system)\n",
    "\n",
    "    # Remove any unused axes\n",
    "    for i in range(len(systems), len(axs)):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "    return fig, axs\n",
    "\n",
    "def block_average(data: np.ndarray, n_blocks: int = 100, block_size: int | None = None) -> np.ndarray:\n",
    "    \"\"\"Block avarage the data using the requested number of blocks.\"\"\"\n",
    "    if n_blocks and block_size is None:\n",
    "        n_samples = len(data)\n",
    "        block_size = n_samples // n_blocks\n",
    "    elif block_size and n_blocks is None:\n",
    "        n_samples = len(data)\n",
    "        n_blocks = n_samples // block_size\n",
    "    else:\n",
    "        raise ValueError(\"Either n_blocks or block_size should be provided.\")\n",
    "    blocks = np.array_split(data[:n_blocks * block_size], n_blocks)\n",
    "    return np.array([np.mean(block) for block in blocks])\n",
    "\n",
    "def block_average_times(times: np.ndarray, n_blocks: int = 100) -> np.ndarray:\n",
    "    \"\"\"Get the times corresponding to block averages.\"\"\"\n",
    "    n_samples = len(times)\n",
    "    block_size = n_samples // n_blocks\n",
    "    return np.array([times[block_size * i] for i in range(n_blocks)])\n",
    "\n",
    "def get_time_idxs(times: np.ndarray, time: float) -> int:\n",
    "    \"\"\"Get the index of the first time greater than the requested time.\"\"\"\n",
    "    return int(np.argmax(times >= time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gradient_arrays_30ns.pkl', 'rb') as f:\n",
    "    gradient_arrays_30ns = pkl.load(f)\n",
    "\n",
    "LAM = 0.45\n",
    "# LAM = 0.95\n",
    "\n",
    "systems = list(gradient_arrays_30ns.keys())\n",
    "n_repeats = gradient_arrays_30ns[systems[0]][\"bound\"][\"vanish\"][0.0][\"grads\"].shape[0]\n",
    "\n",
    "# Get timeseries of the overall free energy change for the bound vanish stages using thermodynamic integration with the trapzoidal rule.\n",
    "# We use the bound vanish stage as it has the most pronounced equliibration behaviour.\n",
    "timeseries_data = {}\n",
    "for system in systems:\n",
    "    all_data = gradient_arrays_30ns[system][\"bound\"][\"vanish\"]\n",
    "    lam_values = list(all_data.keys())\n",
    "    # Times are the same for all lambda windows, so just take the first set of times\n",
    "    times = np.array(all_data[lam_values[0]][\"times\"])\n",
    "    # Integrate over the lambda values to get the overall dgs at each time (kcal mol-1)\n",
    "    dgs = np.zeros([len(times), n_repeats])\n",
    "    for time_idx in range(len(times)):\n",
    "        for repeat_idx in range(n_repeats):\n",
    "            # Atrificially just use one lambda value\n",
    "            dgs[time_idx, repeat_idx] = all_data[LAM][\"grads\"][repeat_idx,time_idx]\n",
    "    timeseries_data[system] = {\"times\": times, \"dgs\": dgs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeseries data for each system\n",
    "\n",
    "fig, axs = get_subplots(systems)\n",
    "for i, system in enumerate(systems):\n",
    "    axs[i].plot(timeseries_data[system][\"times\"], timeseries_data[system][\"dgs\"], alpha=0.5, label=[f\"Repeat {j + 1}\" for j in range(timeseries_data[system][\"dgs\"].shape[1])])\n",
    "    axs[i].plot(timeseries_data[system][\"times\"], np.mean(timeseries_data[system][\"dgs\"], axis=1), color='black', alpha=0.5, label=\"Mean\")\n",
    "    axs[i].set_xlabel(\"Time / ns\")\n",
    "    axs[i].set_ylabel(\"$\\Delta G$ / kcal mol$^{-1}$\")\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.4, 0.8), loc='upper left')\n",
    "fig.savefig(\"output_single/timeseries_dg.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot block-averaged data to better visualise trends\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    # Apply block averaging to each replicate run individually\n",
    "    block_averaged_dgs = np.array([block_average(timeseries_data[system][\"dgs\"][:,j]) for j in range(n_repeats)]).T\n",
    "    times = block_average_times(timeseries_data[system][\"times\"])\n",
    "    axs[i].plot(times, block_averaged_dgs, alpha=0.5, label=[f\"Repeat {j + 1}\" for j in range(n_repeats)])\n",
    "    axs[i].plot(times, np.mean(block_averaged_dgs, axis=1), color='black', alpha=0.8, label=\"Mean\")\n",
    "    axs[i].set_xlabel(\"Time / ns\")\n",
    "    axs[i].set_ylabel(\"$\\Delta G$ / kcal mol$^{-1}$\")\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.4, 0.8), loc='upper left')\n",
    "fig.savefig(\"output_single/timeseries_block_dg.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the data <a name=\"fit\"></a>\n",
    "\n",
    "### Model trends with exponentials <a name=\"exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the cases, take the last 20 ns to be equilibrated. Fit an exponential decay to the data after subtracting the mean value of the last 20 ns.\n",
    "\n",
    "def exp_decay(x: float | np.ndarray, a: float, b: float) -> float:\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "synthetic_data_params = {system:{} for system in systems}\n",
    "idx_10ns = get_time_idxs(timeseries_data[systems[0]][\"times\"], 10)\n",
    "\n",
    "for system in systems:\n",
    "    mean = np.mean(timeseries_data[system][\"dgs\"][idx_10ns:,:])\n",
    "    synthetic_data_params[system][\"equil_region_mean\"] = mean\n",
    "\n",
    "    # Subtract the mean from the data before fitting the exponential decay\n",
    "    shifted_data = timeseries_data[system][\"dgs\"] - mean\n",
    "\n",
    "    # Fit the exponential decay to the data\n",
    "    popt, pcov = scipy.optimize.curve_fit(exp_decay, timeseries_data[system][\"times\"], shifted_data.mean(axis=1), p0=[30, 1.5])\n",
    "    synthetic_data_params[system][\"exp_params\"] = popt\n",
    "\n",
    "    # Calculate the half-life of the exponential decay\n",
    "    synthetic_data_params[system][\"half_life\"] = np.log(2) / popt[1] # in ns\n",
    "\n",
    "    # Subtract the first exponential fit and add another to model the fast initial decay\n",
    "    fit_exp_series = exp_decay(timeseries_data[system][\"times\"], *popt)\n",
    "    # Change the shape of fit_exp_series to match the shape of shifted_data\n",
    "    fit_exp_series = np.tile(fit_exp_series, (n_repeats, 1)).T\n",
    "    shifted_data -= fit_exp_series\n",
    "    popt_fast, pcov_fast = scipy.optimize.curve_fit(exp_decay, timeseries_data[system][\"times\"], shifted_data.mean(axis=1), p0=[30, 0.5])\n",
    "    # If we get a negative a, set it to 0\n",
    "    if popt_fast[0] < 0:\n",
    "        popt_fast[0] = 0\n",
    "        popt_fast[1] = 0\n",
    "    fast_half_life = np.log(2) / popt_fast[1] if popt_fast[1] > 0 else np.inf\n",
    "\n",
    "    # If the fast half-life is greater than the slow half-life, set the fast half-life to infinity and the fast exponential decay to 0\n",
    "    if fast_half_life > synthetic_data_params[system][\"half_life\"]:\n",
    "        popt_fast[0] = 0\n",
    "        popt_fast[1] = 0\n",
    "        fast_half_life = np.inf\n",
    "        \n",
    "    synthetic_data_params[system][\"fast_exp_params\"] = popt_fast\n",
    "    synthetic_data_params[system][\"fast_half_life\"] = fast_half_life\n",
    "\n",
    "\n",
    "    print(30*\"#\")\n",
    "    print(f\"System: {system}\")\n",
    "    print(f\"Half-life: {synthetic_data_params[system]['half_life']:.2f} ns\")\n",
    "    print(f\"Exponential decay parameters: a = {popt[0]:.2f}, b = {popt[1]:.2f}\")\n",
    "    print(f\"Fast half-life: {fast_half_life:.2f} ns\")\n",
    "    print(f\"Fast exponential decay parameters: a = {popt_fast[0]:.2f}, b = {popt_fast[1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the block-averaged mean traces with the exponential decay fit\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    # Apply block averaging to each replicate run individually\n",
    "    block_averaged_dgs = np.array([block_average(timeseries_data[system][\"dgs\"][:,j]) for j in range(n_repeats)]).T\n",
    "    times = block_average_times(timeseries_data[system][\"times\"])\n",
    "    axs[i].plot(times, block_averaged_dgs, alpha=0.5, label=[f\"Repeat {j + 1}\" for j in range(n_repeats)])\n",
    "    axs[i].plot(times, np.mean(block_averaged_dgs, axis=1), color='black', alpha=0.8, label=\"Mean\")\n",
    "    # Plot slow and fast exponential fits, and the combined fit\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='green', label=\"Exponential decay fit\", alpha=0.5)\n",
    "    # Plot the fast exponential fit\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"fast_exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='blue', label=\"Fast exponential decay fit\", alpha=0.5)\n",
    "    # Plot the combined fit\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"exp_params\"]) + exp_decay(times, *synthetic_data_params[system][\"fast_exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='red', label=\"Combined fit\")\n",
    "    axs[i].set_xlabel(\"Time / ns\")\n",
    "    axs[i].set_ylabel(\"$\\Delta G$ / kcal mol$^{-1}$\")\n",
    "    # Label the plot with the exponential decay parameters and half-life\n",
    "    axs[i].text(0.5, 0.9, f\"Half-life: {synthetic_data_params[system]['half_life']:.2f} ns\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "    axs[i].text(0.5, 0.8, f\"a = {synthetic_data_params[system]['exp_params'][0]:.2f} kcal mol$^{{-1}}$\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.3, 0.8), loc='upper left')\n",
    "fig.savefig(\"output_single/timeseries_block_dg_exp_fit.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above, but zoom in on the first ns\n",
    "\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    # Apply block averaging to each replicate run individually\n",
    "    block_averaged_dgs = np.array([block_average(timeseries_data[system][\"dgs\"][:,j], 5000) for j in range(n_repeats)]).T\n",
    "    times = block_average_times(timeseries_data[system][\"times\"], 5000)\n",
    "    axs[i].plot(times, block_averaged_dgs, alpha=0.5, label=[f\"Repeat {j + 1}\" for j in range(n_repeats)])\n",
    "    axs[i].plot(times, np.mean(block_averaged_dgs, axis=1), color='black', alpha=0.8, label=\"Mean\")\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='green', label=\"Exponential decay fit\", alpha=0.5)\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"fast_exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='blue', label=\"Fast exponential decay fit\", alpha=0.5)\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"exp_params\"]) + exp_decay(times, *synthetic_data_params[system][\"fast_exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='red', label=\"Combined fit\")\n",
    "    axs[i].set_xlabel(\"Time / ns\")\n",
    "    axs[i].set_ylabel(\"$\\Delta G$ / kcal mol$^{-1}$\")\n",
    "    axs[i].set_xlim(0, 0.2)\n",
    "    # Label the plot with the exponential decay parameters and half-life\n",
    "    axs[i].text(0.5, 0.9, f\"Half-life: {synthetic_data_params[system]['half_life']:.2f} ns\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "    axs[i].text(0.5, 0.8, f\"a = {synthetic_data_params[system]['exp_params'][0]:.2f} kcal mol$^{{-1}}$\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.3, 0.8), loc='upper left')\n",
    "fig.savefig(\"output_single/timeseries_block_dg_exp_fit_zoom.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model stationary distributions with uncorrelated Gaussian noise <a name=\"noise\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normality(data: np.ndarray, axs: list[plt.Axes]) -> None:\n",
    "    \"\"\"\n",
    "    Plot the histogram and QQ plot for a given set of data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        The data to plot.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Plot the histogram, kernel density estimate, and QQ plot\n",
    "    axs[0].hist(data, edgecolor=\"black\")\n",
    "    sns.kdeplot(data, ax=axs[1], color=\"black\", linewidth=2)\n",
    "    st.probplot(data, plot=axs[2])\n",
    "\n",
    "    # Set the axis labels\n",
    "    axs[0].set_xlabel(\"Value\")\n",
    "    axs[0].set_ylabel(\"Frequency\")\n",
    "    axs[0].set_title(\"Histogram\")\n",
    "    axs[1].set_xlabel(\"Value\")\n",
    "    axs[1].set_ylabel(\"Frequency\")\n",
    "    axs[1].set_title(\"Kernel Density Estimate\")\n",
    "    axs[2].set_xlabel(\"Theoretical Normal Quantiles\")\n",
    "    axs[2].set_ylabel(\"Ordered Values\")\n",
    "    axs[2].set_title(\"QQ Plot\")\n",
    "\n",
    "    # Compute the Shapiro-Wilk test and print the p value\n",
    "    _, p_value = st.shapiro(data)\n",
    "    axs[2].text(\n",
    "        0.5,\n",
    "        0.95,\n",
    "        f\"Shapiro-Wilk p-value: {p_value:.2f}\",\n",
    "        transform=axs[2].transAxes,\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"top\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how Gaussian the stationary distributions are for each system\n",
    "\n",
    "fig, axs = plt.subplots(len(systems), 3, figsize=(12, 4 * len(systems)))\n",
    "for i, system in enumerate(systems):\n",
    "    # Get the stationary distribution for the mean trace\n",
    "    stationary_dgs = timeseries_data[system][\"dgs\"][idx_10ns:,:].mean(axis=1)\n",
    "    # Plot the normality of the stationary distribution\n",
    "    plot_normality(stationary_dgs, axs[i])\n",
    "    axs[i, 0].set_title(f\"{system} Stationary Distribution\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"output_single/stationary_dg_normality.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the autocorrelation of the stationary distributions <a name=\"autocorr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from red.variance import _get_autocovariance, _get_gamma_cap, _get_initial_convex_sequence, _get_initial_positive_sequence, _get_initial_monotone_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the covariance - we take the autocovariance at lag times of 0 and 1 directly from the data, and calculate later lags based on Geyer's initial convex sequence method. This is intended to fit initial lags well (which make large contributions to the autocovariance), while removing effects from later lag times caused by non-stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for system in systems:\n",
    "    # Get the stationary distribution for the mean trace\n",
    "    stationary_dgs = timeseries_data[system][\"dgs\"][idx_10ns:,:].mean(axis=1)\n",
    "    # Reshape into a 2D array, as expected by red\n",
    "    stationary_dgs.reshape(1, -1)\n",
    "    # Get the autocovariance series\n",
    "    autocov_series = _get_autocovariance(stationary_dgs)\n",
    "    # Get the gamma series using Geyer's initial convex sequence method\n",
    "    gamma_series = _get_gamma_cap(autocov_series)[1:]\n",
    "    # Get the initial convex sequence\n",
    "    initial_convex_sequence = _get_initial_convex_sequence(gamma_series)\n",
    "    # Convert the initial convex gamma sequence back into an autocovariance series\n",
    "    # Do this by interpolating, doubling the amount of data\n",
    "    x_interpolate = np.arange(2*len(initial_convex_sequence))\n",
    "    # x_gamma is at x = 0.5, 2.5, 4.5, etc.\n",
    "    x_gamma = np.arange(len(initial_convex_sequence)) * 2 + 0.5\n",
    "    autocov_convex = np.interp(x_interpolate, x_gamma, initial_convex_sequence) / 2\n",
    "    autocov_convex = np.concatenate([autocov_series[:2], autocov_convex])\n",
    "    # Save parameters\n",
    "    synthetic_data_params[system][\"autocov_convex\"] = autocov_convex\n",
    "    synthetic_data_params[system][\"autocov\"] = autocov_series\n",
    "    # Get the total variance (accounting for autocorrelation) from the autocov_convex data\n",
    "    synthetic_data_params[system][\"total_variance\"] = 2 * np.sum(autocov_convex) - autocov_convex[0]\n",
    "    synthetic_data_params[system][\"max_lag_idx\"] = len(autocov_convex) - 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all of the autocovariance series against the initial convex sequences\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "first_idx = 0\n",
    "last_idx = None\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    axs[i].plot(np.arange(len(synthetic_data_params[system][\"autocov\"][first_idx:last_idx])), synthetic_data_params[system][\"autocov\"][first_idx:last_idx], label=\"Autocovariance\")\n",
    "    axs[i].plot(np.arange(len(synthetic_data_params[system][\"autocov_convex\"][first_idx:last_idx])), synthetic_data_params[system][\"autocov_convex\"][first_idx:last_idx], label=\"Initial Convex Sequence\")\n",
    "    axs[i].set_xlabel(\"Lag\")\n",
    "    axs[i].set_ylabel(\"Autocovariance / kcal$^2$ mol$^{-2}$\")\n",
    "    axs[i].set_title(system)\n",
    "    # Add the total variance to the plot\n",
    "    axs[i].text(0.5, 0.7, f\"Total Variance:\\n {synthetic_data_params[system]['total_variance']:.2f} kcal$^2$ mol$^{{-2}}$\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "    # Add max lag index to the plot\n",
    "    axs[i].text(0.5, 0.5, f\"Max Lag Index:\\n {synthetic_data_params[system]['max_lag_idx']}\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.25, 0.7), loc='upper left')\n",
    "fig.savefig(\"output_single/autocovariance_initial_convex_long.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all of the autocovariance series against the initial convex sequences\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "first_idx = 0\n",
    "last_idx = 10\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    axs[i].plot(np.arange(len(synthetic_data_params[system][\"autocov\"][first_idx:last_idx])), synthetic_data_params[system][\"autocov\"][first_idx:last_idx], label=\"Autocovariance\", marker='o')\n",
    "    axs[i].plot(np.arange(len(synthetic_data_params[system][\"autocov_convex\"][first_idx:last_idx])), synthetic_data_params[system][\"autocov_convex\"][first_idx:last_idx], label=\"Initial Convex Sequence\", marker='o')\n",
    "    axs[i].set_xlabel(\"Lag\")\n",
    "    axs[i].set_ylabel(\"Autocovariance / kcal$^2$ mol$^{-2}$\")\n",
    "    axs[i].set_title(system)\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.25, 0.7), loc='upper left')\n",
    "fig.savefig(\"output_single/autocovariance_initial_convex_zoom.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_significant_figures(x, sig_figs=2):\n",
    "    return f\"{x:.{sig_figs}g}\"\n",
    "\n",
    "# Create the DataFrame\n",
    "overview_df = pd.DataFrame(columns=[\"Half-life (ns)\", \"a (kcal mol$^{-1}$)\", \"Fast Half-life (ns)\", \"Fast a (kcal mol$^{-1}$)\", \"Total Variance (kcal$^2$ mol$^{-2}$)\", \"Max Lag Index\"])\n",
    "for system in systems:\n",
    "    overview_df.loc[system] = [synthetic_data_params[system][\"half_life\"], synthetic_data_params[system][\"exp_params\"][0], synthetic_data_params[system][\"fast_half_life\"], synthetic_data_params[system][\"fast_exp_params\"][0], synthetic_data_params[system][\"total_variance\"], synthetic_data_params[system][\"max_lag_idx\"]]\n",
    "\n",
    "# Apply the rounding function to all elements in the DataFrame\n",
    "overview_df = overview_df.applymap(lambda x: format_significant_figures(x, 2))\n",
    "\n",
    "# Save csv and latex\n",
    "overview_df.to_csv(\"output_single/overview.csv\")\n",
    "latex_str = overview_df.to_latex(\"output_single/overview.tex\",index=True, escape=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_single/synthetic_data_params.pkl', 'wb') as f:\n",
    "     pkl.dump(synthetic_data_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic datasets <a name=\"synth\"></a>\n",
    "\n",
    "Generate the following synthetic datasets:\n",
    "\n",
    "- \"Standard\" - Standard synthetic data as described above, up to 8 ns as this is a typical length for an ABFE calculation window\n",
    "- \"Short\" - Use only the first 0.2 ns of data\n",
    "- \"Subsampled\" - Keep only 1 out of every 100 points\n",
    "- \"Noisy\" - Increase the variance by a factor of 5 \n",
    "- \"Block Averaged\" - Decrease the variance by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"FIGURES_ONLY\", False):\n",
    "\n",
    "    def get_cholesky(autocov_fn: np.ndarray, n_data: int) -> np.ndarray:\n",
    "        \"\"\"Get the Cholesky decomposition of the autocovariance function\"\"\"\n",
    "        # Figure out if we need to pad or truncate the autocovariance function\n",
    "        if len(autocov_fn) > n_data:\n",
    "            autocov_fn = autocov_fn[:n_data]\n",
    "        else:\n",
    "            autocov_fn = np.pad(autocov_fn, (0, n_data - len(autocov_fn)), 'constant')\n",
    "\n",
    "        # Generate the autocovariance matrix\n",
    "        autocov_matrix = toeplitz(autocov_fn)\n",
    "\n",
    "        # Get Cholesky decomposition.\n",
    "        cholesky = scipy.linalg.cholesky(autocov_matrix, lower=True)\n",
    "        return cholesky\n",
    "\n",
    "    def generate_data(cholesky_mat: np.ndarray, scale:float = 1) -> np.ndarray:\n",
    "        \"\"\"Generate new data from an autocovariance function\"\"\"\n",
    "        white_noise = np.random.normal(size=cholesky_mat.shape[0], scale=scale)\n",
    "        return np.dot(cholesky_mat, white_noise)\n",
    "\n",
    "\n",
    "    def generate_data_with_transient(cholesky_mat: np.ndarray,\n",
    "                                    times: np.ndarray,\n",
    "                                    slow_exponential_params: np.ndarray,\n",
    "                                    fast_exponential_params: np.ndarray,\n",
    "                                    scale: float = 1) -> np.ndarray:\n",
    "        \"\"\"Generate synthetic data for equilibration detection with an exponential transient\"\"\"\n",
    "        # Get the exponential transient\n",
    "        n_data= cholesky_mat.shape[0]\n",
    "        slow_transient_data = exp_decay(times, *slow_exponential_params)\n",
    "        fast_transient_data = exp_decay(times, *fast_exponential_params)\n",
    "\n",
    "        # Get the stationary data\n",
    "        stationary_data = generate_data(cholesky_mat, scale=scale)\n",
    "\n",
    "        # Add the two together\n",
    "        return stationary_data + slow_transient_data + fast_transient_data\n",
    "    # Generate data up to 8 ns\n",
    "\n",
    "    SET_NAME = \"standard\"\n",
    "    N_REPEATS = 1000\n",
    "    IDX = get_time_idxs(timeseries_data[systems[0]][\"times\"], 8)\n",
    "\n",
    "    synthetic_data_bound_vanish = {}\n",
    "    synthetic_data_bound_vanish[SET_NAME] = {}\n",
    "\n",
    "    for system in tqdm.tqdm(systems, desc=system):\n",
    "        synthetic_data_bound_vanish[SET_NAME][system] = {}\n",
    "        autocov_fn = synthetic_data_params[system][\"autocov_convex\"]\n",
    "        cholesky_mat = get_cholesky(autocov_fn, IDX)\n",
    "        times = timeseries_data[system][\"times\"][:IDX]\n",
    "        synthetic_data_bound_vanish[SET_NAME][\"times\"] = times\n",
    "        for i in range(N_REPEATS):\n",
    "            data = generate_data_with_transient(cholesky_mat,\n",
    "                                                times,\n",
    "                                                synthetic_data_params[system][\"exp_params\"],\n",
    "                                                synthetic_data_params[system][\"fast_exp_params\"])\n",
    "            synthetic_data_bound_vanish[SET_NAME][system][i] = {}\n",
    "            synthetic_data_bound_vanish[SET_NAME][system][i][\"data\"] = data\n",
    "\n",
    "    with open(\"output_single/synthetic_data_bound_vanish.pkl\", \"wb\") as f:\n",
    "        pkl.dump(synthetic_data_bound_vanish, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "red",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
