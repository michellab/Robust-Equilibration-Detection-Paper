{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic Datasets for Testing Using the Free Vanish Stage\n",
    "\n",
    "In order to test the performance of equilibration detection methods, we create sets of synthetic datasets modelled on long ABFE runs for six different systems (see the 30 ns non-adaptive runs [here](https://chemrxiv.org/engage/chemrxiv/article-details/6670b524c9c6a5c07aafa972) for details). Here, we create synthetic data based on the free vanish stage of an ABFE calculation.\n",
    "\n",
    "This notebook is organised as follows:\n",
    "\n",
    "- [Load the data](#load)\n",
    "- [Fit the data](#fit)\n",
    "    - [Model trends with exponentials](#exp)\n",
    "    - [Model stationary distributions with uncorrelated Gaussian noise](#noise)\n",
    "    - [Reintroduce desired correlation structure with Cholesky decomposition](#cholesky)\n",
    "- [Create synthetic datasets](#synth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import red\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from pymbar import timeseries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from scipy.linalg import toeplitz\n",
    "import seaborn as sns\n",
    "\n",
    "# Set ggplot style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def get_subplots(systems: list[str]) -> tuple[plt.Figure, list[plt.Axes]]:\n",
    "    # Plot two columns side-by-side\n",
    "    n_cols = min(2, len(systems))\n",
    "    n_rows = int(np.ceil(len(systems) / n_cols))\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3*n_rows))\n",
    "    if len(systems) == 1:\n",
    "        axs = [axs]\n",
    "    else:\n",
    "        axs = axs.flatten()\n",
    "\n",
    "    # Set titles\n",
    "    for i, system in enumerate(systems):\n",
    "        axs[i].set_title(system)\n",
    "\n",
    "    # Remove any unused axes\n",
    "    for i in range(len(systems), len(axs)):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "    return fig, axs\n",
    "\n",
    "def block_average(data: np.ndarray, n_blocks: int = 100) -> np.ndarray:\n",
    "    \"\"\"Block avarage the data using the requested number of blocks.\"\"\"\n",
    "    n_samples = len(data)\n",
    "    block_size = n_samples // n_blocks\n",
    "    blocks = np.array_split(data[:n_blocks * block_size], n_blocks)\n",
    "    return np.array([np.mean(block) for block in blocks])\n",
    "\n",
    "def block_average_times(times: np.ndarray, n_blocks: int = 100) -> np.ndarray:\n",
    "    \"\"\"Get the times corresponding to block averages.\"\"\"\n",
    "    n_samples = len(times)\n",
    "    block_size = n_samples // n_blocks\n",
    "    return np.array([times[block_size * i] for i in range(n_blocks)])\n",
    "\n",
    "def get_time_idxs(times: np.ndarray, time: float) -> int:\n",
    "    \"\"\"Get the index of the first time greater than the requested time.\"\"\"\n",
    "    return int(np.argmax(times >= time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gradient_arrays_30ns.pkl', 'rb') as f:\n",
    "    gradient_arrays_30ns = pkl.load(f)\n",
    "\n",
    "systems = list(gradient_arrays_30ns.keys())\n",
    "n_repeats = gradient_arrays_30ns[systems[0]][\"free\"][\"vanish\"][0.0][\"grads\"].shape[0]\n",
    "\n",
    "# Get timeseries of the overall free energy change for the bound vanish stages using thermodynamic integration with the trapzoidal rule.\n",
    "# We use the bound vanish stage as it has the most pronounced equliibration behaviour.\n",
    "timeseries_data = {}\n",
    "for system in systems:\n",
    "    all_data = gradient_arrays_30ns[system][\"free\"][\"vanish\"]\n",
    "    lam_values = list(all_data.keys())\n",
    "    # Times are the same for all lambda windows, so just take the first set of times\n",
    "    times = np.array(all_data[lam_values[0]][\"times\"])\n",
    "    # Integrate over the lambda values to get the overall dgs at each time (kcal mol-1)\n",
    "    dgs = np.zeros([len(times), n_repeats])\n",
    "    for lam_idx in range(len(times)):\n",
    "        for repeat_idx in range(n_repeats):\n",
    "            dgs[lam_idx, repeat_idx] = np.trapz([all_data[lam][\"grads\"][repeat_idx,lam_idx] for lam in lam_values], x=lam_values)\n",
    "    timeseries_data[system] = {\"times\": times, \"dgs\": dgs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeseries data for each system\n",
    "\n",
    "fig, axs = get_subplots(systems)\n",
    "for i, system in enumerate(systems):\n",
    "    axs[i].plot(timeseries_data[system][\"times\"], timeseries_data[system][\"dgs\"], alpha=0.5, label=[f\"Repeat {j + 1}\" for j in range(timeseries_data[system][\"dgs\"].shape[1])])\n",
    "    axs[i].plot(timeseries_data[system][\"times\"], np.mean(timeseries_data[system][\"dgs\"], axis=1), color='black', alpha=0.5, label=\"Mean\")\n",
    "    axs[i].set_xlabel(\"Time / ns\")\n",
    "    axs[i].set_ylabel(\"$\\Delta G$ / kcal mol$^{-1}$\")\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.4, 0.8), loc='upper left')\n",
    "fig.savefig(\"output_free/timeseries_dg.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot block-averaged data to better visualise trends\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    # Apply block averaging to each replicate run individually\n",
    "    block_averaged_dgs = np.array([block_average(timeseries_data[system][\"dgs\"][:,j]) for j in range(n_repeats)]).T\n",
    "    times = block_average_times(timeseries_data[system][\"times\"])\n",
    "    axs[i].plot(times, block_averaged_dgs, alpha=0.5, label=[f\"Repeat {j + 1}\" for j in range(n_repeats)])\n",
    "    axs[i].plot(times, np.mean(block_averaged_dgs, axis=1), color='black', alpha=0.8, label=\"Mean\")\n",
    "    axs[i].set_xlabel(\"Time / ns\")\n",
    "    axs[i].set_ylabel(\"$\\Delta G$ / kcal mol$^{-1}$\")\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.4, 0.8), loc='upper left')\n",
    "fig.savefig(\"output_free/timeseries_block_dg.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the data <a name=\"fit\"></a>\n",
    "\n",
    "### Model trends with exponentials <a name=\"exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the cases, take the last 20 ns to be equilibrated. Fit an exponential decay to the data after subtracting the mean value of the last 20 ns.\n",
    "\n",
    "def exp_decay(x: float, a: float, b: float) -> float:\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "synthetic_data_params = {system:{} for system in systems}\n",
    "idx_10ns = get_time_idxs(timeseries_data[systems[0]][\"times\"], 10)\n",
    "\n",
    "for system in systems:\n",
    "    mean = np.mean(timeseries_data[system][\"dgs\"][idx_10ns:,:])\n",
    "    synthetic_data_params[system][\"equil_region_mean\"] = mean\n",
    "\n",
    "    # Subtract the mean from the data before fitting the exponential decay\n",
    "    shifted_data = timeseries_data[system][\"dgs\"] - mean\n",
    "\n",
    "    # Fit the exponential decay to the data\n",
    "    popt, pcov = scipy.optimize.curve_fit(exp_decay, timeseries_data[system][\"times\"], shifted_data.mean(axis=1), p0=[30, 1.5])\n",
    "    synthetic_data_params[system][\"exp_params\"] = popt\n",
    "    \n",
    "    # Calculate the half-life of the exponential decay\n",
    "    synthetic_data_params[system][\"half_life\"] = np.log(2) / popt[1] # in ns\n",
    "\n",
    "    # If the half life is ridiculously fast, assume no initial transient and avoid fitting more exponentials\n",
    "    if synthetic_data_params[system][\"half_life\"] < 0.05:\n",
    "        # Zero current exponential parameters\n",
    "        synthetic_data_params[system][\"exp_params\"] = [0, 0]\n",
    "        synthetic_data_params[system][\"half_life\"] = np.inf\n",
    "        # Zero fast exponential parameters\n",
    "        fast_half_life = np.inf\n",
    "        popt_fast = [0, 0]\n",
    "        synthetic_data_params[system][\"fast_exp_params\"] = popt_fast\n",
    "        synthetic_data_params[system][\"fast_half_life\"] = fast_half_life\n",
    "\n",
    "    else:\n",
    "        # Subtract the first exponential fit and add another to model the fast initial decay\n",
    "        fit_exp_series = exp_decay(timeseries_data[system][\"times\"], *popt)\n",
    "        # Change the shape of fit_exp_series to match the shape of shifted_data\n",
    "        fit_exp_series = np.tile(fit_exp_series, (n_repeats, 1)).T\n",
    "        shifted_data -= fit_exp_series\n",
    "        popt_fast, pcov_fast = scipy.optimize.curve_fit(exp_decay, timeseries_data[system][\"times\"], shifted_data.mean(axis=1), p0=[30, 0.5])\n",
    "        # If we get a negative a, set it to 0\n",
    "        if popt_fast[0] < 0:\n",
    "            popt_fast[0] = 0\n",
    "            popt_fast[1] = 0\n",
    "        fast_half_life = np.log(2) / popt_fast[1] if popt_fast[1] > 0 else np.inf\n",
    "        synthetic_data_params[system][\"fast_exp_params\"] = popt_fast\n",
    "        synthetic_data_params[system][\"fast_half_life\"] = fast_half_life\n",
    "\n",
    "\n",
    "    print(30*\"#\")\n",
    "    print(f\"System: {system}\")\n",
    "    print(f\"Half-life: {synthetic_data_params[system]['half_life']:.2f} ns\")\n",
    "    print(f\"Exponential decay parameters: a = {popt[0]:.2f}, b = {popt[1]:.2f}\")\n",
    "    print(f\"Fast half-life: {fast_half_life:.2f} ns\")\n",
    "    print(f\"Fast exponential decay parameters: a = {popt_fast[0]:.2f}, b = {popt_fast[1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the block-averaged mean traces with the exponential decay fit\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    # Apply block averaging to each replicate run individually\n",
    "    block_averaged_dgs = np.array([block_average(timeseries_data[system][\"dgs\"][:,j]) for j in range(n_repeats)]).T\n",
    "    times = block_average_times(timeseries_data[system][\"times\"])\n",
    "    axs[i].plot(times, block_averaged_dgs, alpha=0.5, label=[f\"Repeat {j + 1}\" for j in range(n_repeats)])\n",
    "    axs[i].plot(times, np.mean(block_averaged_dgs, axis=1), color='black', alpha=0.8, label=\"Mean\")\n",
    "    # Plot slow and fast exponential fits, and the combined fit\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='green', label=\"Exponential decay fit\", alpha=0.5)\n",
    "    # Plot the fast exponential fit\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"fast_exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='blue', label=\"Fast exponential decay fit\", alpha=0.5)\n",
    "    # Plot the combined fit\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"exp_params\"]) + exp_decay(times, *synthetic_data_params[system][\"fast_exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='red', label=\"Combined fit\")\n",
    "    axs[i].set_xlabel(\"Time / ns\")\n",
    "    axs[i].set_ylabel(\"$\\Delta G$ / kcal mol$^{-1}$\")\n",
    "    # Label the plot with the exponential decay parameters and half-life\n",
    "    axs[i].text(0.5, 0.9, f\"Half-life: {synthetic_data_params[system]['half_life']:.2f} ns\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "    axs[i].text(0.5, 0.8, f\"a = {synthetic_data_params[system]['exp_params'][0]:.2f} kcal mol$^{{-1}}$\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.3, 0.8), loc='upper left')\n",
    "fig.savefig(\"output_free/timeseries_block_dg_exp_fit.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above, but zoom in on the first ns\n",
    "\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    # Apply block averaging to each replicate run individually\n",
    "    block_averaged_dgs = np.array([block_average(timeseries_data[system][\"dgs\"][:,j], 5000) for j in range(n_repeats)]).T\n",
    "    times = block_average_times(timeseries_data[system][\"times\"], 5000)\n",
    "    axs[i].plot(times, block_averaged_dgs, alpha=0.5, label=[f\"Repeat {j + 1}\" for j in range(n_repeats)])\n",
    "    axs[i].plot(times, np.mean(block_averaged_dgs, axis=1), color='black', alpha=0.8, label=\"Mean\")\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='green', label=\"Exponential decay fit\", alpha=0.5)\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"fast_exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='blue', label=\"Fast exponential decay fit\", alpha=0.5)\n",
    "    axs[i].plot(times, exp_decay(times, *synthetic_data_params[system][\"exp_params\"]) + exp_decay(times, *synthetic_data_params[system][\"fast_exp_params\"]) + synthetic_data_params[system][\"equil_region_mean\"], color='red', label=\"Combined fit\")\n",
    "    axs[i].set_xlabel(\"Time / ns\")\n",
    "    axs[i].set_ylabel(\"$\\Delta G$ / kcal mol$^{-1}$\")\n",
    "    axs[i].set_xlim(0, 0.2)\n",
    "    # Label the plot with the exponential decay parameters and half-life\n",
    "    axs[i].text(0.5, 0.9, f\"Half-life: {synthetic_data_params[system]['half_life']:.2f} ns\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "    axs[i].text(0.5, 0.8, f\"a = {synthetic_data_params[system]['exp_params'][0]:.2f} kcal mol$^{{-1}}$\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.3, 0.8), loc='upper left')\n",
    "fig.savefig(\"output_free/timeseries_block_dg_exp_fit_zoom.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model stationary distributions with uncorrelated Gaussian noise <a name=\"noise\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normality(data: np.ndarray, axs: list[plt.Axes]) -> None:\n",
    "    \"\"\"\n",
    "    Plot the histogram and QQ plot for a given set of data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        The data to plot.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Plot the histogram, kernel density estimate, and QQ plot\n",
    "    axs[0].hist(data, edgecolor=\"black\")\n",
    "    sns.kdeplot(data, ax=axs[1], color=\"black\", linewidth=2)\n",
    "    st.probplot(data, plot=axs[2])\n",
    "\n",
    "    # Set the axis labels\n",
    "    axs[0].set_xlabel(\"Value\")\n",
    "    axs[0].set_ylabel(\"Frequency\")\n",
    "    axs[0].set_title(\"Histogram\")\n",
    "    axs[1].set_xlabel(\"Value\")\n",
    "    axs[1].set_ylabel(\"Frequency\")\n",
    "    axs[1].set_title(\"Kernel Density Estimate\")\n",
    "    axs[2].set_xlabel(\"Theoretical Normal Quantiles\")\n",
    "    axs[2].set_ylabel(\"Ordered Values\")\n",
    "    axs[2].set_title(\"QQ Plot\")\n",
    "\n",
    "    # Compute the Shapiro-Wilk test and print the p value\n",
    "    _, p_value = st.shapiro(data)\n",
    "    axs[2].text(\n",
    "        0.5,\n",
    "        0.95,\n",
    "        f\"Shapiro-Wilk p-value: {p_value:.2f}\",\n",
    "        transform=axs[2].transAxes,\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"top\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how Gaussian the stationary distributions are for each system\n",
    "\n",
    "fig, axs = plt.subplots(len(systems), 3, figsize=(12, 4 * len(systems)))\n",
    "for i, system in enumerate(systems):\n",
    "    # Get the stationary distribution for the mean trace\n",
    "    stationary_dgs = timeseries_data[system][\"dgs\"][idx_10ns:,:].mean(axis=1)\n",
    "    # Plot the normality of the stationary distribution\n",
    "    plot_normality(stationary_dgs, axs[i])\n",
    "    axs[i, 0].set_title(f\"{system} Stationary Distribution\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"output_free/stationary_dg_normality.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the autocorrelation of the stationary distributions <a name=\"autocorr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from red.variance import _get_autocovariance, _get_gamma_cap, _get_initial_convex_sequence, _get_initial_positive_sequence, _get_initial_monotone_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the covariance - we take the autocovariance at lag times of 0 and 1 directly from the data, and calculate later lags based on Geyer's initial convex sequence method. This is intended to fit initial lags well (which make large contributions to the autocovariance), while removing effects from later lag times caused by non-stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for system in systems:\n",
    "    # Get the stationary distribution for the mean trace\n",
    "    stationary_dgs = timeseries_data[system][\"dgs\"][idx_10ns:,:].mean(axis=1)\n",
    "    # Reshape into a 2D array, as expected by red\n",
    "    stationary_dgs.reshape(1, -1)\n",
    "    # Get the autocovariance series\n",
    "    autocov_series = _get_autocovariance(stationary_dgs)\n",
    "    # Get the gamma series using Geyer's initial convex sequence method\n",
    "    gamma_series = _get_gamma_cap(autocov_series)[1:]\n",
    "    # Get the initial convex sequence\n",
    "    initial_convex_sequence = _get_initial_convex_sequence(gamma_series)\n",
    "    # Convert the initial convex gamma sequence back into an autocovariance series\n",
    "    # Do this by interpolating, doubling the amount of data\n",
    "    x_interpolate = np.arange(2*len(initial_convex_sequence))\n",
    "    # x_gamma is at x = 0.5, 2.5, 4.5, etc.\n",
    "    x_gamma = np.arange(len(initial_convex_sequence)) * 2 + 0.5\n",
    "    autocov_convex = np.interp(x_interpolate, x_gamma, initial_convex_sequence) / 2\n",
    "    autocov_convex = np.concatenate([autocov_series[:2], autocov_convex])\n",
    "    # Save parameters\n",
    "    synthetic_data_params[system][\"autocov_convex\"] = autocov_convex\n",
    "    synthetic_data_params[system][\"autocov\"] = autocov_series\n",
    "    # Get the total variance (accounting for autocorrelation) from the autocov_convex data\n",
    "    synthetic_data_params[system][\"total_variance\"] = 2 * np.sum(autocov_convex) - autocov_convex[0]\n",
    "    synthetic_data_params[system][\"max_lag_idx\"] = len(autocov_convex) - 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all of the autocovariance series against the initial convex sequences\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "first_idx = 0\n",
    "last_idx = None\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    axs[i].plot(np.arange(len(synthetic_data_params[system][\"autocov\"][first_idx:last_idx])), synthetic_data_params[system][\"autocov\"][first_idx:last_idx], label=\"Autocovariance\")\n",
    "    axs[i].plot(np.arange(len(synthetic_data_params[system][\"autocov_convex\"][first_idx:last_idx])), synthetic_data_params[system][\"autocov_convex\"][first_idx:last_idx], label=\"Initial Convex Sequence\")\n",
    "    axs[i].set_xlabel(\"Lag\")\n",
    "    axs[i].set_ylabel(\"Autocovariance / kcal$^2$ mol$^{-2}$\")\n",
    "    axs[i].set_title(system)\n",
    "    # Add the total variance to the plot\n",
    "    axs[i].text(0.5, 0.7, f\"Total Variance:\\n {synthetic_data_params[system]['total_variance']:.2f} kcal$^2$ mol$^{{-2}}$\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "    # Add max lag index to the plot\n",
    "    axs[i].text(0.5, 0.5, f\"Max Lag Index:\\n {synthetic_data_params[system]['max_lag_idx']}\", horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.25, 0.7), loc='upper left')\n",
    "fig.savefig(\"output_free/autocovariance_initial_convex_long.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all of the autocovariance series against the initial convex sequences\n",
    "fig, axs = get_subplots(systems)\n",
    "\n",
    "first_idx = 0\n",
    "last_idx = 10\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    axs[i].plot(np.arange(len(synthetic_data_params[system][\"autocov\"][first_idx:last_idx])), synthetic_data_params[system][\"autocov\"][first_idx:last_idx], label=\"Autocovariance\", marker='o')\n",
    "    axs[i].plot(np.arange(len(synthetic_data_params[system][\"autocov_convex\"][first_idx:last_idx])), synthetic_data_params[system][\"autocov_convex\"][first_idx:last_idx], label=\"Initial Convex Sequence\", marker='o')\n",
    "    axs[i].set_xlabel(\"Lag\")\n",
    "    axs[i].set_ylabel(\"Autocovariance / kcal$^2$ mol$^{-2}$\")\n",
    "    axs[i].set_title(system)\n",
    "\n",
    "fig.tight_layout()\n",
    "axs[-2].legend(bbox_to_anchor=(1.25, 0.7), loc='upper left')\n",
    "fig.savefig(\"output_free/autocovariance_initial_convex_zoom.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overview of all systems - include exponential parameters (half-life and a), total variance, and max lag index\n",
    "\n",
    "overview_df = pd.DataFrame(columns=[\"Half-life (ns)\", \"a (kcal mol$^{-1}$)\", \"Fast Half-life (ns)\", \"Fast a (kcal mol$^{-1}$)\", \"Total Variance (kcal$^2$ mol$^{-2}$)\", \"Max Lag Index\"])\n",
    "for system in systems:\n",
    "    overview_df.loc[system] = [synthetic_data_params[system][\"half_life\"], synthetic_data_params[system][\"exp_params\"][0], synthetic_data_params[system][\"fast_half_life\"], synthetic_data_params[system][\"fast_exp_params\"][0], synthetic_data_params[system][\"total_variance\"], synthetic_data_params[system][\"max_lag_idx\"]]\n",
    "overview_df.to_csv(\"output_free/overview.csv\")\n",
    "overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_significant_figures(x, sig_figs=2):\n",
    "    return f\"{x:.{sig_figs}f}\"\n",
    "\n",
    "# Create the DataFrame\n",
    "overview_df = pd.DataFrame(columns=[\"Half-life (ns)\", \"a (kcal mol$^{-1}$)\", \"Fast Half-life (ns)\", \"Fast a (kcal mol$^{-1}$)\", \"Total Variance (kcal$^2$ mol$^{-2}$)\", \"Max Lag Index\"])\n",
    "for system in systems:\n",
    "    overview_df.loc[system] = [synthetic_data_params[system][\"half_life\"], synthetic_data_params[system][\"exp_params\"][0], synthetic_data_params[system][\"fast_half_life\"], synthetic_data_params[system][\"fast_exp_params\"][0], synthetic_data_params[system][\"total_variance\"], synthetic_data_params[system][\"max_lag_idx\"]]\n",
    "\n",
    "# Apply the rounding function to all elements in the DataFrame\n",
    "overview_df = overview_df.applymap(lambda x: format_significant_figures(x, 2))\n",
    "\n",
    "# Save csv and latex\n",
    "overview_df.to_csv(\"output_free/overview.csv\")\n",
    "latex_str = overview_df.to_latex(\"output_free/overview.tex\",index=True, escape=False)\n",
    "\n",
    "overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_free/synthetic_data_params.pkl', 'wb') as f:\n",
    "    pkl.dump(synthetic_data_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic datasets <a name=\"synth\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the following synthetic datasets:\n",
    "\n",
    "- \"Standard\" - Standard synthetic data as described above, up to 8 ns as this is a typical length for an ABFE calculation window\n",
    "\n",
    "Here, we want an example of both a fairly uncorrelated dataset and a fairly correlated dataset. For the first, we'll use benzene, and for the last we'll use the PDE2A ligand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cholesky(autocov_fn: np.ndarray, n_data: int) -> np.ndarray:\n",
    "    \"\"\"Get the Cholesky decomposition of the autocovariance function\"\"\"\n",
    "    # Figure out if we need to pad or truncate the autocovariance function\n",
    "    if len(autocov_fn) > n_data:\n",
    "        autocov_fn = autocov_fn[:n_data]\n",
    "    else:\n",
    "        autocov_fn = np.pad(autocov_fn, (0, n_data - len(autocov_fn)), 'constant')\n",
    "\n",
    "    # Generate the autocovariance matrix\n",
    "    autocov_matrix = toeplitz(autocov_fn)\n",
    "\n",
    "    # Get Cholesky decomposition.\n",
    "    cholesky = scipy.linalg.cholesky(autocov_matrix, lower=True)\n",
    "    return cholesky\n",
    "\n",
    "def generate_data(cholesky_mat: np.ndarray, scale:float = 1) -> np.ndarray:\n",
    "    \"\"\"Generate new data from an autocovariance function\"\"\"\n",
    "    white_noise = np.random.normal(size=cholesky_mat.shape[0], scale=scale)\n",
    "    return np.dot(cholesky_mat, white_noise)\n",
    "\n",
    "\n",
    "def generate_data_with_transient(cholesky_mat: np.ndarray,\n",
    "                                 times: np.ndarray,\n",
    "                                 slow_exponential_params: np.ndarray,\n",
    "                                 fast_exponential_params: np.ndarray,\n",
    "                                 scale: float = 1) -> np.ndarray:\n",
    "    \"\"\"Generate synthetic data for equilibration detection with an exponential transient\"\"\"\n",
    "    # Get the exponential transient\n",
    "    n_data= cholesky_mat.shape[0]\n",
    "    slow_transient_data = exp_decay(times, *slow_exponential_params)\n",
    "    fast_transient_data = exp_decay(times, *fast_exponential_params)\n",
    "\n",
    "    # Get the stationary data\n",
    "    stationary_data = generate_data(cholesky_mat, scale=scale)\n",
    "\n",
    "    # Add the two together\n",
    "    return stationary_data + slow_transient_data + fast_transient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data up to 8 ns\n",
    "\n",
    "SET_NAME = \"standard\"\n",
    "N_REPEATS = 1000\n",
    "IDX = get_time_idxs(timeseries_data[systems[0]][\"times\"], 8)\n",
    "\n",
    "synthetic_data_free_vanish = {}\n",
    "synthetic_data_free_vanish[SET_NAME] = {}\n",
    "\n",
    "for system in tqdm.tqdm([\"T4L\", \"PDE2A\"]):\n",
    "    synthetic_data_free_vanish[SET_NAME][system] = {}\n",
    "    autocov_fn = synthetic_data_params[system][\"autocov_convex\"]\n",
    "    cholesky_mat = get_cholesky(autocov_fn, IDX)\n",
    "    times = timeseries_data[system][\"times\"][:IDX]\n",
    "    synthetic_data_free_vanish[SET_NAME][\"times\"] = times\n",
    "    for i in range(N_REPEATS):\n",
    "        data = generate_data_with_transient(cholesky_mat,\n",
    "                                            times,\n",
    "                                            synthetic_data_params[system][\"exp_params\"],\n",
    "                                            synthetic_data_params[system][\"fast_exp_params\"])\n",
    "        synthetic_data_free_vanish[SET_NAME][system][i] = {}\n",
    "        synthetic_data_free_vanish[SET_NAME][system][i][\"data\"] = data\n",
    "\n",
    "# with open(\"output_free/synthetic_data_free_vanish.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(synthetic_data_free_vanish, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import red\n",
    "from pymbar import timeseries\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "# with open(\"output_free/synthetic_data_free_vanish.pkl\", \"rb\") as f:\n",
    "with open(\"output/synthetic_data_bound_vanish.pkl\", \"rb\") as f:\n",
    "    synthetic_data_free_vanish = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acovf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def standard_cov(data):\n",
    "    n_samples = len(data)\n",
    "    max_lag = n_samples - 1\n",
    "    return np.correlate(data, data, mode='full')[n_samples-1:n_samples+max_lag] / n_samples\n",
    "\n",
    "def slow_standard_cov(data):\n",
    "    n_samples = len(data)\n",
    "    max_lag = n_samples - 1\n",
    "        # Initialise the auto-correlation function.\n",
    "    auto_cov = np.zeros(max_lag + 1)\n",
    "\n",
    "    # Calculate the auto-correlation function.\n",
    "    auto_cov[0] = data.dot(data)\n",
    "    for t in range(1, max_lag + 1):\n",
    "        auto_cov[t] = data[t:].dot(data[:-t])\n",
    "        # auto_cov[t] = _np.sum(data[t:] * data[:-t])\n",
    "    auto_cov /= n_samples  # \"Biased\" estimate, rather than n - 1.\n",
    "\n",
    "    return auto_cov\n",
    "\n",
    "def fft_cov(data):\n",
    "    n_samples = len(data)\n",
    "    max_lag = n_samples - 1\n",
    "    return acovf(data, adjusted=False, nlag=max_lag, fft=True, demean=False)\n",
    "\n",
    "# Time both of the above functions against data of increasing length\n",
    "\n",
    "import time\n",
    "\n",
    "full_data = synthetic_data_free_vanish[\"standard\"][\"PDE2A\"][0][\"data\"]\n",
    "total_samples = len(full_data)\n",
    "\n",
    "n_samples = np.arange(200, 800, 1)\n",
    "# n_samples = np.arange(100, , 1)\n",
    "times_standard = []\n",
    "times_slow_standard = []\n",
    "times_fft = []\n",
    "\n",
    "for n in n_samples:\n",
    "    data = full_data[:n]\n",
    "    start = time.time()\n",
    "    standard_cov(data)\n",
    "    end = time.time()\n",
    "    times_standard.append(end - start)\n",
    "\n",
    "    start = time.time()\n",
    "    fft_cov(data)\n",
    "    end = time.time()\n",
    "    times_fft.append(end - start)\n",
    "\n",
    "    start = time.time()\n",
    "    slow_standard_cov(data)\n",
    "    end = time.time()\n",
    "    times_slow_standard.append(end - start)\n",
    "\n",
    "# Plot the times\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_samples, times_standard, label=\"Standard Covariance\")\n",
    "ax.plot(n_samples, times_fft, label=\"FFT Covariance\")\n",
    "ax.plot(n_samples, times_slow_standard, label=\"Slow Standard Covariance\")\n",
    "ax.set_xlabel(\"Number of Samples\")\n",
    "ax.set_ylabel(\"Time / s\")\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = synthetic_data_free_vanish[\"standard\"][\"PDE2A\"][0][\"data\"]\n",
    "total_samples = len(full_data)\n",
    "\n",
    "n_samples = np.arange(100, total_samples, 100)\n",
    "times_standard = []\n",
    "times_fft = []\n",
    "\n",
    "for n in n_samples:\n",
    "    data = full_data[:n]\n",
    "    start = time.time()\n",
    "    standard_cov(data)\n",
    "    end = time.time()\n",
    "    times_standard.append(end - start)\n",
    "\n",
    "    start = time.time()\n",
    "    fft_cov(data)\n",
    "    end = time.time()\n",
    "    times_fft.append(end - start)\n",
    "\n",
    "# Plot the times\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_samples, times_standard, label=\"Standard Covariance\")\n",
    "ax.plot(n_samples, times_fft, label=\"FFT Covariance\")\n",
    "ax.set_xlabel(\"Number of Samples\")\n",
    "ax.set_ylabel(\"Time / s\")\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time t, g, neff = red.detect_equilibration_init_seq(synthetic_data_free_vanish[\"standard\"][\"PDE2A\"][2][\"data\"][:], method=\"max_ess\", sequence_estimator=\"initial_convex\", plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time t,g, neff = timeseries.detectEquilibration(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:], fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time t, g, neff = red.detect_equilibration_init_seq(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:], method=\"max_ess\", sequence_estimator=\"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time t, g, neff = red.detect_equilibration_window(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit t,g, neff = timeseries.detectEquilibration(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit t,g, neff = timeseries.detectEquilibration(synthetic_data_free_vanish[SET_NAME][\"T4L\"][0][\"data\"], fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit t, g, neff = red.detect_equilibration_init_seq(synthetic_data_free_vanish[SET_NAME][\"T4L\"][0][\"data\"], method=\"max_ess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(synthetic_data_free_vanish[SET_NAME][\"T4L\"][0][\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun t,g, neff = timeseries.detectEquilibration(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:10000], fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun t, g, neff = red.detect_equilibration_init_seq(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as _np\n",
    "from typing import Union as _Union\n",
    "from copy import deepcopy as _deepcopy\n",
    "\n",
    "\n",
    "def _get_autocovariance(\n",
    "    data: _np.ndarray,\n",
    "    max_lag: _Union[None, int] = None,\n",
    "    mean: _Union[None, float] = None,\n",
    ") -> _np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the auto-covariance as a function of lag time for a time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        A time series of data with shape (n_samples,).\n",
    "\n",
    "    max_lag : int, optional, default=None\n",
    "        The maximum lag time to use when calculating the auto-correlation function.\n",
    "        If None, the maximum lag time will be the length of the time series.\n",
    "        The default is None.\n",
    "\n",
    "    mean: float, optional, default=None\n",
    "        The mean of the time series. If None, the mean will be calculated from the\n",
    "        time series. This is useful when the mean has been calculated from an\n",
    "        ensemble of time series.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The auto-correlation function of the time series.\n",
    "    \"\"\"\n",
    "    # Copy the data so we don't modify the original.\n",
    "    data = _deepcopy(data)\n",
    "\n",
    "    # Get the length of the time series.\n",
    "    n_samples: int = data.shape[0]\n",
    "\n",
    "    # If max_lag_time is None, set it to the length of the time series.\n",
    "    if max_lag is None:\n",
    "        max_lag = n_samples - 1\n",
    "\n",
    "    # If mean is None, calculate it from the time series.\n",
    "    if mean is None:\n",
    "        mean = data.mean()\n",
    "\n",
    "    # Subtract the mean from the data.\n",
    "    data -= mean  # type: ignore\n",
    "\n",
    "    return _np.correlate(data, data, mode='full')[n_samples-1:n_samples+max_lag] / n_samples\n",
    "\n",
    "    # Initialise the auto-correlation function.\n",
    "    auto_cov = _np.zeros(max_lag + 1)\n",
    "\n",
    "    # Calculate the auto-correlation function.\n",
    "    auto_cov[0] = data.dot(data)\n",
    "    for t in range(1, max_lag + 1):\n",
    "        auto_cov[t] = data[t:].dot(data[:-t])\n",
    "        # auto_cov[t] = _np.sum(data[t:] * data[:-t])\n",
    "    auto_cov /= n_samples  # \"Biased\" estimate, rather than n - 1.\n",
    "\n",
    "    return auto_cov\n",
    "\n",
    "_get_autocovariance(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acovf, acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acovf(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:], fft=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acovf(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:], fft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit acovf(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:], fft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _get_autocovariance(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acovf(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:5000], fft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun _get_autocovariance(synthetic_data_free_vanish[\"standard\"][\"T4L\"][0][\"data\"][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_inefficiency(A_n, B_n=None, fast=False, mintime=3, fft=False):\n",
    "    \"\"\"Compute the (cross) statistical inefficiency of (two) timeseries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A_n : np.ndarray, float\n",
    "        A_n[n] is nth value of timeseries A.  Length is deduced from vector.\n",
    "    B_n : np.ndarray, float, optional, default=None\n",
    "        B_n[n] is nth value of timeseries B.  Length is deduced from vector.\n",
    "        If supplied, the cross-correlation of timeseries A and B will be estimated instead of the\n",
    "        autocorrelation of timeseries A.\n",
    "    fast : bool, optional, default=False\n",
    "        f True, will use faster (but less accurate) method to estimate correlation\n",
    "        time, described in Ref. [1] (default: False).  This is ignored\n",
    "        when B_n=None and fft=True.\n",
    "    mintime : int, optional, default=3\n",
    "        minimum amount of correlation function to compute (default: 3)\n",
    "        The algorithm terminates after computing the correlation time out to mintime when the\n",
    "        correlation function first goes negative.  Note that this time may need to be increased\n",
    "        if there is a strong initial negative peak in the correlation function.\n",
    "    fft : bool, optional, default=False\n",
    "        If fft=True and B_n=None, then use the fft based approach, as\n",
    "        implemented in statistical_inefficiency_fft().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    g : np.ndarray,\n",
    "        g is the estimated statistical inefficiency (equal to 1 + 2 tau, where tau is the correlation time).\n",
    "        We enforce g >= 1.0.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The same timeseries can be used for both A_n and B_n to get the autocorrelation statistical inefficiency.\n",
    "    The fast method described in Ref [1] is used to compute g.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n",
    "    histogram analysis method for the analysis of simulated and parallel tempering simulations.\n",
    "    JCTC 3(1):26-41, 2007.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    Compute statistical inefficiency of timeseries data with known correlation time.\n",
    "\n",
    "    >>> from pymbar.testsystems import correlated_timeseries_example\n",
    "    >>> A_n = correlated_timeseries_example(N=100000, tau=5.0)\n",
    "    >>> g = statistical_inefficiency(A_n, fast=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create numpy copies of input arguments.\n",
    "    A_n = np.array(A_n)\n",
    "\n",
    "    if fft and B_n is None:\n",
    "        return statistical_inefficiency_fft(A_n, mintime=mintime)\n",
    "\n",
    "    if B_n is not None:\n",
    "        B_n = np.array(B_n)\n",
    "    else:\n",
    "        B_n = np.array(A_n)\n",
    "\n",
    "    # Get the length of the timeseries.\n",
    "    N = A_n.size\n",
    "\n",
    "    # Be sure A_n and B_n have the same dimensions.\n",
    "    if A_n.shape != B_n.shape:\n",
    "        raise ParameterError(\"A_n and B_n must have same dimensions.\")\n",
    "\n",
    "    # Initialize statistical inefficiency estimate with uncorrelated value.\n",
    "    g = 1.0\n",
    "\n",
    "    # Compute mean of each timeseries.\n",
    "    mu_A = A_n.mean()\n",
    "    mu_B = B_n.mean()\n",
    "\n",
    "    # Make temporary copies of fluctuation from mean.\n",
    "    dA_n = A_n.astype(np.float64) - mu_A\n",
    "    dB_n = B_n.astype(np.float64) - mu_B\n",
    "\n",
    "    # Compute estimator of covariance of (A,B) using estimator that will ensure C(0) = 1.\n",
    "    sigma2_AB = (dA_n * dB_n).mean()  # standard estimator to ensure C(0) = 1\n",
    "\n",
    "    # Trap the case where this covariance is zero, and we cannot proceed.\n",
    "    if sigma2_AB == 0:\n",
    "        raise ParameterError(\n",
    "            \"Sample covariance sigma_AB^2 = 0 -- cannot compute statistical inefficiency\"\n",
    "        )\n",
    "\n",
    "    # Accumulate the integrated correlation time by computing the normalized correlation time at\n",
    "    # increasing values of t.  Stop accumulating if the correlation function goes negative, since\n",
    "    # this is unlikely to occur unless the correlation function has decayed to the point where it\n",
    "    # is dominated by noise and indistinguishable from zero.\n",
    "    t = 1\n",
    "    increment = 1\n",
    "    while t < N - 1:\n",
    "        # compute normalized fluctuation correlation function at time t\n",
    "        C = np.sum(dA_n[0 : (N - t)] * dB_n[t:N] + dB_n[0 : (N - t)] * dA_n[t:N]) / (\n",
    "            2.0 * float(N - t) * sigma2_AB\n",
    "        )\n",
    "\n",
    "        # Terminate if the correlation function has crossed zero and we've computed the correlation\n",
    "        # function at least out to 'mintime'.\n",
    "        if (C <= 0.0) and (t > mintime):\n",
    "            break\n",
    "\n",
    "        # Accumulate contribution to the statistical inefficiency.\n",
    "        g += 2.0 * C * (1.0 - float(t) / float(N)) * float(increment)\n",
    "\n",
    "        # Increment t and the amount by which we increment t.\n",
    "        t += increment\n",
    "\n",
    "        # Increase the interval if \"fast mode\" is on.\n",
    "        if fast:\n",
    "            increment += 1\n",
    "\n",
    "    # g must be at least unity\n",
    "    if g < 1.0:\n",
    "        g = 1.0\n",
    "\n",
    "    # Return the computed statistical inefficiency.\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit statistical_inefficiency(synthetic_data_free_vanish[\"standard\"][\"PDE2A\"][0][\"data\"][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _get_autocovariance(synthetic_data_free_vanish[\"standard\"][\"PDE2A\"][0][\"data\"][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit t, g, neff = red.detect_equilibration_window(synthetic_data_free_vanish[SET_NAME][\"T4L\"][0][\"data\"], method=\"max_ess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how Chodera's method varies on just 10 of the outputs\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "N_IT = 50\n",
    "SET_NAME = \"standard\"\n",
    "SYSTEM = \"PDE2A\"\n",
    "\n",
    "fracs_discarded = np.zeros(N_IT)\n",
    "for i in tqdm(range(N_IT)):\n",
    "    data = synthetic_data_free_vanish[SET_NAME][SYSTEM][\"data\"]\n",
    "    t, g, Neff = timeseries.detectEquilibration(data)\n",
    "    frac_discarded = t/len(data)\n",
    "    fracs_discarded[i] = frac_discarded\n",
    "    print(f\"Fraction discarded: {frac_discarded:.2f}, t={t}, g={g}, Neff={Neff}\")\n",
    "\n",
    "# Plot distribution of fractions discarded\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(fracs_discarded, ax=ax, kde=True)\n",
    "ax.set_xlabel(\"Fraction Discarded\")\n",
    "ax.set_ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how Chodera's method varies on just 10 of the outputs\n",
    "\n",
    "fracs_discarded = np.zeros(N_IT)\n",
    "for i in tqdm(range(N_IT)):\n",
    "    data = synthetic_data_free_vanish[SET_NAME][SYSTEM][i][\"data\"]\n",
    "    t, g, Neff = timeseries.detectEquilibration(data, fast=False)\n",
    "    frac_discarded = t/len(data)\n",
    "    fracs_discarded[i] = frac_discarded\n",
    "    print(f\"Fraction discarded: {frac_discarded:.2f}, t={t}, g={g}, Neff={Neff}\")\n",
    "\n",
    "# Plot distribution of fractions discarded\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(fracs_discarded, ax=ax, kde=True)\n",
    "ax.set_xlabel(\"Fraction Discarded\")\n",
    "ax.set_ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fracs_discarded = np.zeros(N_IT)\n",
    "for i in range(N_IT):\n",
    "    data = synthetic_data_free_vanish[SET_NAME][SYSTEM][i][\"data\"]\n",
    "    t, g, neff = red.detect_equilibration_init_seq(data, method=\"max_ess\", sequence_estimator=\"positive\")\n",
    "    frac_discarded = t/len(data)\n",
    "    fracs_discarded[i] = frac_discarded\n",
    "    print(f\"Fraction discarded: {frac_discarded:.2f}, t={t}, g={g}, Neff={Neff}\")\n",
    "\n",
    "# Plot distribution of fractions discarded\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(fracs_discarded, ax=ax, kde=True)\n",
    "ax.set_xlabel(\"Fraction Discarded\")\n",
    "ax.set_ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fracs_discarded = np.zeros(N_IT)\n",
    "for i in range(N_IT):\n",
    "    data = synthetic_data_free_vanish[SET_NAME][SYSTEM][i][\"data\"]\n",
    "    t, g, neff = red.detect_equilibration_init_seq(data, method=\"min_sse\", sequence_estimator=\"positive\")\n",
    "    frac_discarded = t/len(data)\n",
    "    fracs_discarded[i] = frac_discarded\n",
    "    print(f\"Fraction discarded: {frac_discarded:.2f}, t={t}, g={g}, Neff={Neff}\")\n",
    "\n",
    "# Plot distribution of fractions discarded\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(fracs_discarded, ax=ax, kde=True)\n",
    "ax.set_xlabel(\"Fraction Discarded\")\n",
    "ax.set_ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how Chodera's method varies on just 10 of the outputs\n",
    "\n",
    "N_IT = 50\n",
    "\n",
    "fracs_discarded = np.zeros(N_IT)\n",
    "for i in range(N_IT):\n",
    "    data = synthetic_data_bound_vanish[SET_NAME][system][i][\"data\"]\n",
    "    t, g, Neff = timeseries.detectEquilibration(data)\n",
    "    frac_discarded = t/len(data)\n",
    "    fracs_discarded[i] = frac_discarded\n",
    "    print(f\"Fraction discarded: {frac_discarded:.2f}, t={t}, g={g}, Neff={Neff}\")\n",
    "\n",
    "# Plot distribution of fractions discarded\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(fracs_discarded, ax=ax, kde=True)\n",
    "ax.set_xlabel(\"Fraction Discarded\")\n",
    "ax.set_ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, g1, neff1 = timeseries.detectEquilibration(data, fast=False)\n",
    "t2, g2, neff2 = red.detect_equilibration_init_seq(data, method=\"max_ess\", sequence_estimator=\"positive\")\n",
    "assert t1 == t2\n",
    "# assert g1 == g2\n",
    "assert neff1 == neff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "red",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
